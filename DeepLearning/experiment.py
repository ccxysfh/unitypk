#!/usr/bin/env python
# encoding: utf-8

"""
@author: changxin
@software: PyCharm
@file: experiment.py
@time: 26/12/2017 13:19
"""
"""
æ„å»ºäº†æ¯ä¸ªç²’å­çš„ç‰¹å¾å‘é‡åï¼Œä»¥å½“å‰çš„åŠ é€Ÿåº¦ä¸ºæ ‡è®°å€¼ï¼Œæˆ–è€…ä»¥å½“å‰çš„åŠ é€Ÿåº¦è¡¨è¾¾å¯†åº¦ğ›’ï¼Œç›®æ ‡å‡½æ•°ä¸ºï¼ˆğ›’-ğ›’0ï¼‰
å‡ä½œå°è¯•

1. é’ˆå¯¹æ¯ä¸€å¸§çš„æ¯ä¸€ä¸ªç²’å­éƒ½éœ€è¦æ ¹æ®ç²’å­çš„åæ ‡è¿›è¡Œé‡‡æ ·ï¼Œé‡‡æ ·æ–¹å¼åŠé‡‡æ ·æ•°ç›®å¾…å®šï¼Œæ„å»ºç‰¹å¾å‘é‡
2. æ˜¯å¦éœ€è¦è¿›è¡Œé™ç»´ï¼Œå…·ä½“ç‰¹å¾å‘é‡æ„å»ºå‡ºæ¥ä¹‹åå†è¡Œå¤„ç†
3. æ¯ä¸€ä¸ªepochå¦‚ä½•æŒ‰batch_sizeè¿›è¡Œè®­ç»ƒæ ·æœ¬çš„è·å–
4. è¾¹ç•Œä¸æ»‘ç§»æ¡ä»¶å¦‚ä½•ä¿è¯
5. ä½ ä»æ¥æ²¡æœ‰ææ¸…æ¥šè¿‡æŸå¤±å‡½æ•°ä¸åŒæƒ…å†µæ˜¯æ€æ ·çš„ï¼ŒåŒºåˆ†å›å½’å’Œåˆ†ç±»é—®é¢˜ï¼ŒäºŒåˆ†ç±»å¤šåˆ†ç±»ï¼Œå•å€¼å›å½’å¤šå€¼å›å½’ï¼Œä»¥åŠä¸åŒçš„æŸå¤±è®¡é‡æ–¹æ³•æ€»ç»“
"""
import random
import os

import numpy as np
import tensorflow as tf

data_dir_path = "/data/datasets/simulation_data/frames"

def reset_graph(seed=42):
    tf.reset_default_graph()
    tf.set_random_seed(seed)
    np.random.seed(seed)
reset_graph()

def sample_by_filename(filename):
    # selected pirticle

    file_path = os.path.join(data_dir_path, filename)
    frame_particle = np.loadtxt(file_path, dtype=np.str, delimiter=",")
    # å¯†åº¦ï¼Œéœ€è¦è¿›è¡Œé‡‡æ ·ï¼Œä½†è¿™é‡Œå¯¹æ‰€æœ‰çš„ç²’å­è¿›è¡Œäº†æ±‚è§£
    f = open("feature_vectors.csv", "w+")
    f_labels = open("feature_vectors_labels.csv", "w+")
    number_particles = len(frame_particle)
    for i in range(number_particles):
        f.writelines(str(i))
        f.writelines(",")
        for num in range(100):
            theta = random.random()
            for j in range(number_particles):
                if i == j:
                    continue

                if j == number_particles - 1:
                    j = number_particles - 100

                if j == i + 1:
                    # è¡¨é¢å¼ åŠ›
                    delta_px = theta * (float(frame_particle[i, 0]) - float(frame_particle[j, 0]))
                    delta_py = theta * (float(frame_particle[i, 1]) - float(frame_particle[j, 1]))
                    delta_pz = theta * (float(frame_particle[i, 2]) - float(frame_particle[j, 2]))

                    # ç²˜æ€§åŠ›
                    delta_vx = theta * (float(frame_particle[i, 0]) - float(frame_particle[j, 0]))
                    delta_vy = theta * (float(frame_particle[i, 1]) - float(frame_particle[j, 1]))
                    delta_vz = theta * (float(frame_particle[i, 2]) - float(frame_particle[j, 2]))

                    # å¯†åº¦
                    density = theta * (delta_px + delta_py + delta_pz)

                    # å‹å¼ºåŠ›
                    px = theta * (density * delta_px)
                    py = theta * (density * delta_py)
                    pz = theta * (density * delta_pz)

                    all_props = [delta_px, delta_py, delta_pz, delta_vx, delta_vy, delta_vz, density, px, py, pz]
                    all_props = ",".join(list(map(str, all_props)))

            f.writelines(all_props)
            f.writelines(',')
            f.flush()
        f.writelines('\n')
    # f_labels.write(",".join(list(map(str,[i,frame_particle[i,9], frame_particle[i,10], frame_particle[i,11]]))))
    f.close()

from tensorflow.contrib.layers import fully_connected
def net_structure(X_train=None,y_train=None,X_valid=None,y_valid=None,X_test=None,y_test=None):
    n_inputs = X_train.shape[1]
    n_outputs = y_train.shape[1]


    from datetime import datetime

    def log_dir(prefix=""):
        now = datetime.utcnow().strftime("%Y%m%d%H%M%S")
        root_logdir = "tf_logs"
        if prefix:
            prefix += "-"
        name = prefix + "run-" + now
        return "{}/{}/".format(root_logdir, name)
    graph = tf.Graph()
    with graph.as_default():
        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")
        y = tf.placeholder(tf.float32, shape=(None), name="y")

        he_init = tf.contrib.layers.variance_scaling_initializer()
        # TODO ä¿®æ”¹éšè—å±‚åŠæ¯å±‚ç¥ç»å…ƒä¸ªæ•°
        def dnn(inputs, n_hidden_layers=5, n_neurons=15, name=None, activation=tf.nn.elu, initializer=he_init):
            with tf.name_scope("dnn"):
                for layer in range(n_hidden_layers):
                    inputs = tf.layers.dense(inputs, n_neurons, activation=activation, \
                                             kernel_initializer=initializer, name="hidden%d" % (layer + 1))
            return inputs

        dnn_outputs = dnn(X)
        logits = fully_connected(dnn_outputs, n_outputs, scope="outputs", activation_fn=None)

        """
        å¯¹äºæŸå¤±å‡½æ•°çš„å®šä¹‰å°šæœ‰ç–‘è™‘ï¼Œå†è¡Œè€ƒè™‘,ç›®å‰å§‘ä¸”å…ˆä½¿ç”¨å‡æ–¹å·®å§
        """
        with tf.name_scope("train"):
            loss = tf.reduce_mean(np.multiply(logits-y, logits-y))
            optimizer = tf.train.AdamOptimizer()
            training_op = optimizer.minimize(loss)
        """
        æ·»åŠ summaryï¼Œä»¥è°ƒç”¨tensorboardè¿›è¡Œæ£€æµ‹
        """


        logdir = log_dir("fluids")
        with tf.name_scope("summary"):
            loss_summary = tf.summary.scalar("log_loss", loss)
            file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())

        init = tf.global_variables_initializer()
        saver = tf.train.Saver()
    print("Build graph end.")
    n_epochs = 1000
    batch_size = 20

    # needed in case of early stopping
    max_checks_without_progress = 20
    checks_without_progress = 0
    best_loss = np.infty
    best_params = None

    checkpoint_path = "/tmp/my_fluid_model.ckpt"
    checkpoint_epoch_path = checkpoint_path + ".epoch"
    final_model_path = "./my_fluid_model"

    # X_train = ""
    # y_train = ""
    # X_valid = ""
    # y_valid = ""
    # X_test = ""
    # y_test = ""


    session = tf.Session(graph=graph)

    def get_model_params(session, graph):
        """Get all variable values (used for early stopping, faster than saving to disk)"""
        with graph.as_default():
            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
        return {gvar.op.name: value for gvar, value in zip(gvars, session.run(gvars))}

    def restore_model_params(session, graph, model_params):
        """Set all variables to the given values (for early stopping, faster than loading from disk)"""
        gvar_names = list(model_params.keys())
        assign_ops = {gvar_name: graph.get_operation_by_name(gvar_name + "/Assign")
                      for gvar_name in gvar_names}
        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}
        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}
        session.run(assign_ops, feed_dict=feed_dict)

    with session.as_default() as sess:
        if not os.path.exists(checkpoint_path):
            start_epoch = 0
            sess.run(init)
            print("start epoch 0")
        else:
            with open(checkpoint_epoch_path, 'rb') as f:
                start_epoch = int(f.read())
            print("Training was interrupted. Continuing at epoch", start_epoch)
            saver.restore(sess, final_model_path)
        for epoch in range(start_epoch, n_epochs):
            rnd_idx = np.random.permutation(len(X_train))
            for rnd_indices in np.array_split(rnd_idx, len(X_train) // batch_size):
                X_batch, y_batch = X_train[rnd_indices], y_train[rnd_indices]
                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})

            loss_evl, summary_str = sess.run([loss, loss_summary], feed_dict={X: X_test, y: y_test})
            file_writer.add_summary(summary_str, epoch)
            print(epoch, "loss:", loss_evl)
            if epoch % 5 == 0:
                with open(checkpoint_epoch_path, 'wb') as f:
                    saver.save(sess, checkpoint_path)
                    f.write(b'%d' % (epoch + 1))
            # early stop
            if X_valid is not None and y_valid is not None:
                loss_val = sess.run(loss, feed_dict={X: X_valid, y: y_valid})
                if loss_val < best_loss:
                    best_params = get_model_params(sess, sess.graph)
                    best_loss = loss_val
                    checks_without_progress = 0
                else:
                    checks_without_progress += 1

                print("{}\tValidation loss: {:.6f}\tBest loss: {:.6f}".format(
                    epoch, loss_val, best_loss))

                if checks_without_progress > max_checks_without_progress:
                    print("Early stopping!")
                    break;
            else:
                loss_train = sess.run(loss, feed_dict={X: X_batch, y: y_batch})

                print("{}\tLast training batch loss: {:.6f}".format(epoch, loss_train))


        saver.save(sess, final_model_path)
        os.remove(checkpoint_epoch_path)

        if best_params:
            restore_model_params(best_params)


def object_function():
    """
    å¾ˆæ˜ç¡®çš„ï¼Œæˆ‘éœ€è¦å®Œå–„ä¸€ä»¶äº‹ï¼Œå¯¹äºè¡¡é‡æœ€ç»ˆç»“æœçš„å¤„ç†å¸¸ç”¨å‡½æ•°çš„æ€»ç»“
    å¦‚åˆ†ç±»ç»“æœå’Œæ ‡ç­¾çš„å¯¹æ¯”
    """
    y = "åˆ†ç±»æ ‡ç­¾"
    y_ = "é¢„æµ‹æ ‡ç­¾"
    tf.argmax(y, 1) # æœ€å¤§å€¼æ‰€åœ¨çš„ç´¢å¼•ä½ç½®å°±æ˜¯ç±»åˆ«æ ‡ç­¾,ç¬¬äºŒä¸ªå‚æ•°è¡¨æ˜çš„æ˜¯axisï¼Œ
    #è¾“å…¥çš„ç»´åº¦ï¼Œæ›´ä¸€èˆ¬çš„0ä¸ºç‹­ä¹‰çš„xè½´ï¼Œ1ä¸ºç‹­ä¹‰çš„yè½´ï¼ŒæŒ‡å‡ºè¡¡é‡tensorçš„å“ªä¸€ç»´åº¦ï¼Œå‘é‡ä¼ å…¥0
    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
    '''
      ä¸ºäº†ç¡®å®šæ­£ç¡®é¢„æµ‹é¡¹çš„æ¯”ä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠå¸ƒå°”å€¼è½¬æ¢æˆæµ®ç‚¹æ•°ï¼Œç„¶åå–å¹³å‡å€¼ã€‚
      ä¾‹å¦‚ï¼Œ [True, False, True, True] ä¼šå˜æˆ [1,0,1,1] ï¼Œå–å¹³å‡å€¼åå¾—åˆ° 0.75.
      castå°†boolè½¬ä¸º1ï¼Œ0
    '''
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

    # clssify

    # regression
    return

def show_tensorboard(logdir):
    '''
    tensorboard --logdir=/path/to/log-directory
    :param log_dir:
    :return:
    '''
    training_op = ""
    merged_summary_op = tf.merge_all_summaries()
    total_step = 0
    epochs = 10000
    with tf.Session() as sess:
        summary_writer = tf.train.SummaryWriter('/tmp/mnist_logs', sess.graph)
        for epoch in range(epochs):
            total_step += 1
            sess.run(training_op)
            if total_step % 100 == 0:
                summary_str = sess.run(merged_summary_op)
                summary_writer.add_summary(summary_str, total_step)
    return

if __name__ == '__main__':
    pass