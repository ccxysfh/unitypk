{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1., 1., size=[m, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sample_Z(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.32981263,  0.05004392,  0.47627667,  0.13431565],\n       [-0.16194942,  0.22798318, -0.39744505,  0.55331022],\n       [ 0.23684063, -0.35169532, -0.34942995,  0.51977966]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(-1., 1., size=[3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.27464269,  0.71796398,  0.48779483, -0.85104084],\n       [ 0.55720824, -0.19462209, -0.47613508, -0.33506048],\n       [ 0.40081993, -0.84522717,  0.80286316,  0.90950975]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "def dnn(inputs, n_hidden_layers=1, n_neurons=128, pre_name=None, activation=tf.nn.elu, \n",
    "        initializer=he_init, reuse=None):\n",
    "    if pre_name == None:\n",
    "        pre_name = \"\"\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        for layer in range(n_hidden_layers):\n",
    "            inputs = tf.layers.dense(inputs, n_neurons, activation=activation, \\\n",
    "                    kernel_initializer=initializer, name=\"%shidden%d\" % (pre_name, layer + 1), reuse=reuse)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_D_inputs = 28*28\n",
    "n_G_inputs = 100\n",
    "\n",
    "n_D_outputs = 1\n",
    "n_G_outputs = 28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#该函数将给出权重初始化的方法\n",
    "def variable_init(size):\n",
    "    in_dim = size[0]\n",
    "\n",
    "    #计算随机生成变量所服从的正态分布标准差\n",
    "    w_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=w_stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义输入矩阵的占位符，输入层单元为784，None代表批量大小的占位，X代表输入的真实图片。占位符的数值类型为32位浮点型\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "\n",
    "#定义判别器的权重矩阵和偏置项向量，由此可知判别网络为三层全连接网络\n",
    "D_W1 = tf.Variable(variable_init([784, 128]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "D_W2 = tf.Variable(variable_init([128, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "#定义生成器的输入噪声为100维度的向量组，None根据批量大小确定\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "\n",
    "#定义生成器的权重与偏置项。输入层为100个神经元且接受随机噪声，\n",
    "#输出层为784个神经元，并输出手写字体图片。生成网络根据原论文为三层全连接网络\n",
    "G_W1 = tf.Variable(variable_init([100, 128]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "G_W2 = tf.Variable(variable_init([128, 784]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = tf.placeholder(tf.float32, shape=(None, n_G_inputs), name=\"Z\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_D_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(Z):\n",
    "    # g_latent_out = dnn(Z, pre_name=\"g_\")\n",
    "    # g_logits = tf.layers.dense(g_latent_out, n_G_outputs, kernel_initializer=he_init, name=\"g_logits\")\n",
    "    # g_proba = tf.nn.sigmoid(g_logits, name=\"g_proba\")\n",
    "    G_h1 = tf.nn.relu(tf.matmul(Z, G_W1) + G_b1)\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "    g_logits  = G_log_prob\n",
    "    g_proba = G_prob\n",
    "    # return g_logits, g_proba\n",
    "    return G_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(X, reuse=None):\n",
    "    # d_latent_out = dnn(X, pre_name=\"d_\",reuse=reuse)\n",
    "    # d_logits = tf.layers.dense(d_latent_out, n_D_outputs, kernel_initializer=he_init, name=\"d_logits\", reuse=reuse)\n",
    "    # d_proba = tf.nn.sigmoid(d_logits)\n",
    "    D_h1 = tf.nn.relu(tf.matmul(X, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "    d_logits = D_logit\n",
    "    d_proba = D_prob\n",
    "    # return d_logits, d_proba\n",
    "    return D_prob, D_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwe need the variables for each layers of models\\ng_proba is G(Z)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "we need the variables for each layers of models\n",
    "g_proba is G(Z)\n",
    "\"\"\"\n",
    "g_logits, g_proba = generator(Z)\n",
    "d_logits_data, d_proba_data = discriminator(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_logits_g, d_proba_g = discriminator(g_proba, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a28c9d78eeb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mVariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOBAL_VARIABLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtheta_G\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtheta_D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "Variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "theta_G = Variables[:4]\n",
    "theta_D = Variables[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入随机噪声z而输出生成样本\n",
    "g_proba = generator(Z)\n",
    "\n",
    "#分别输入真实图片和生成的图片，并投入判别器以判断真伪\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "D_fake, D_logit_fake = discriminator(g_proba)\n",
    "\n",
    "#以下为原论文的判别器损失和生成器损失，但本实现并没有使用该损失函数\n",
    "# D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))\n",
    "# G_loss = -tf.reduce_mean(tf.log(D_fake))\n",
    "\n",
    "# 我们使用交叉熵作为判别器和生成器的损失函数，因为sigmoid_cross_entropy_with_logits内部会对预测输入执行Sigmoid函数，\n",
    "#所以我们取判别器最后一层未投入激活函数的值，即D_h1*D_W2+D_b2。\n",
    "#tf.ones_like(D_logit_real)创建维度和D_logit_real相等的全是1的标注，真实图片。\n",
    "loss_data = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "loss_g = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "\n",
    "#损失函数为两部分，即E[log(D(x))]+E[log(1-D(G(z)))]，将真的判别为假和将假的判别为真\n",
    "loss_D = loss_data + loss_g\n",
    "\n",
    "#同样使用交叉熵构建生成器损失函数\n",
    "loss_G = loss_g\n",
    "\n",
    "#定义判别器和生成器的优化方法为Adam算法，关键字var_list表明最小化损失函数所更新的权重矩阵\n",
    "training_op_d = tf.train.AdamOptimizer().minimize(loss_D, var_list=theta_D)\n",
    "training_op_g = tf.train.AdamOptimizer().minimize(loss_G, var_list=theta_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xentropy_g = tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_g, labels=tf.ones_like(d_logits_g))\n",
    "loss_g = tf.reduce_mean(xentropy_g, name=\"loss_g\")\n",
    "xentropy_data = tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_data, labels=tf.ones_like(d_logits_data))\n",
    "loss_data = tf.reduce_mean(xentropy_data, name=\"loss_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = tf.ones_like([[1,-1,-1],[1,1,-1]])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ones_val = sess.run(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_D = loss_data + loss_g\n",
    "loss_G = tf.reduce_mean(xentropy_g, name=\"loss_G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_op_d = tf.train.AdamOptimizer().minimize(loss_D, var_list=theta_D)\n",
    "training_op_g = tf.train.AdamOptimizer().minimize(loss_G, var_list=theta_G)\n",
    "# training_op_d = optimizer_d.minimize(loss_D, var_list=\n",
    "# theta_D)\n",
    "# training_op_g = optimizer_g.minimize(loss_G, var_list=theta_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20000\n",
    "batch_size = 128\n",
    "z_dim = 100\n",
    "\n",
    "checkpoint_path = \"/tmp/my_mnistGAN_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_mnistGAN_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/changxin/Documents/deeplearning/UnityPK/datasets/MNIST/train-images-idx3-ubyte.gz\nExtracting /Users/changxin/Documents/deeplearning/UnityPK/datasets/MNIST/train-labels-idx1-ubyte.gz\nExtracting /Users/changxin/Documents/deeplearning/UnityPK/datasets/MNIST/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/changxin/Documents/deeplearning/UnityPK/datasets/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/Users/changxin/Documents/deeplearning/UnityPK/datasets/MNIST/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE PHOTO\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\nD loss: 1.751\nG_loss: 0.1543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 2000\nD loss: 1.925e-06\nG_loss: 5.106e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4000\nD loss: 2.502e-07\nG_loss: 9.521e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 6000\nD loss: 1.952e-07\nG_loss: 2.842e-08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 8000\nD loss: 1.321e-08\nG_loss: 9.317e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 10000\nD loss: 1.286e-08\nG_loss: 3.043e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 12000\nD loss: 1.342e-09\nG_loss: 1.218e-09\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # if not os.path.exists(checkpoint_epoch_path):\n",
    "    #     start_epoch = 0\n",
    "    #     sess.run(init)\n",
    "    # else:\n",
    "    #     with open(checkpoint_epoch_path, 'rb') as f:\n",
    "    #         start_epoch = int(f.read())\n",
    "    #     print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "    #     saver.restore(sess, checkpoint_path)\n",
    "    i = 0 \n",
    "    # for epoch in range(start_epoch, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        \n",
    "        X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        # Z_batch = sample_Z(batch_size, z_dim)\n",
    "        _, loss_D_val = sess.run([training_op_d, loss_D], feed_dict={X: X_batch, Z: sample_Z(batch_size, z_dim)})\n",
    "        _, loss_G_val = sess.run([training_op_g, loss_G], feed_dict={Z: sample_Z(batch_size, z_dim)})\n",
    "        \n",
    "        if epoch % 2000 == 0:\n",
    "            samples = sess.run(g_proba, feed_dict={Z: sample_Z(16, z_dim)})\n",
    "            fig = plot(samples)\n",
    "            plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "            i += 1\n",
    "            plt.close(fig)\n",
    "            \n",
    "        # if epoch % 5000 == 0:\n",
    "        #     with open(checkpoint_epoch_path, 'wb') as f:\n",
    "        #         saver.save(sess, checkpoint_path)\n",
    "        #         f.write(b'%d' % (epoch + 1))\n",
    "        \n",
    "        if epoch % 2000 == 0:\n",
    "            print('Iter: {}'.format(epoch))\n",
    "            print('D loss: {:.4}'. format(loss_D_val))\n",
    "            print('G_loss: {:.4}'.format(loss_G_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/changxin/Documents/deeplearning/UnityPK'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.abspath(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
