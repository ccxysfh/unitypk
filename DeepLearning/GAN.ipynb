{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "reset_graph()\n",
    "#该函数将给出权重初始化的方法\n",
    "def variable_init(size):\n",
    "    in_dim = size[0]\n",
    "\n",
    "    #计算随机生成变量所服从的正态分布标准差\n",
    "    w_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=w_stddev)\n",
    "\n",
    "#定义输入矩阵的占位符，输入层单元为784，None代表批量大小的占位，X代表输入的真实图片。占位符的数值类型为32位浮点型\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "\n",
    "#定义判别器的权重矩阵和偏置项向量，由此可知判别网络为三层全连接网络\n",
    "D_W1 = tf.Variable(variable_init([784, 128]))\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "D_W2 = tf.Variable(variable_init([128, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "#定义生成器的输入噪声为100维度的向量组，None根据批量大小确定\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "\n",
    "#定义生成器的权重与偏置项。输入层为100个神经元且接受随机噪声，\n",
    "#输出层为784个神经元，并输出手写字体图片。生成网络根据原论文为三层全连接网络\n",
    "G_W1 = tf.Variable(variable_init([100, 128]))\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "G_W2 = tf.Variable(variable_init([128, 784]))\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
    "\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2]\n",
    "\n",
    "def generator(Z):\n",
    "    # g_latent_out = dnn(Z, pre_name=\"g_\")\n",
    "    # g_logits = tf.layers.dense(g_latent_out, n_G_outputs, kernel_initializer=he_init, name=\"g_logits\")\n",
    "    # g_proba = tf.nn.sigmoid(g_logits, name=\"g_proba\")\n",
    "    G_h1 = tf.nn.relu(tf.matmul(Z, G_W1) + G_b1)\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "    g_logits  = G_log_prob\n",
    "    g_proba = G_prob\n",
    "    # return g_logits, g_proba\n",
    "    return G_prob\n",
    "\n",
    "def discriminator(X, reuse=None):\n",
    "    # d_latent_out = dnn(X, pre_name=\"d_\",reuse=reuse)\n",
    "    # d_logits = tf.layers.dense(d_latent_out, n_D_outputs, kernel_initializer=he_init, name=\"d_logits\", reuse=reuse)\n",
    "    # d_proba = tf.nn.sigmoid(d_logits)\n",
    "    D_h1 = tf.nn.relu(tf.matmul(X, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "    d_logits = D_logit\n",
    "    d_proba = D_prob\n",
    "    # return d_logits, d_proba\n",
    "    return D_prob, D_logit\n",
    "\n",
    "#输入随机噪声z而输出生成样本\n",
    "g_proba = generator(Z)\n",
    "\n",
    "#分别输入真实图片和生成的图片，并投入判别器以判断真伪\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "D_fake, D_logit_fake = discriminator(g_proba)\n",
    "\n",
    "#以下为原论文的判别器损失和生成器损失，但本实现并没有使用该损失函数\n",
    "# D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1. - D_fake))\n",
    "# G_loss = -tf.reduce_mean(tf.log(D_fake))\n",
    "\n",
    "# 我们使用交叉熵作为判别器和生成器的损失函数，因为sigmoid_cross_entropy_with_logits内部会对预测输入执行Sigmoid函数，\n",
    "#所以我们取判别器最后一层未投入激活函数的值，即D_h1*D_W2+D_b2。\n",
    "#tf.ones_like(D_logit_real)创建维度和D_logit_real相等的全是1的标注，真实图片。\n",
    "loss_data = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real)))\n",
    "loss_g = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake)))\n",
    "\n",
    "#损失函数为两部分，即E[log(D(x))]+E[log(1-D(G(z)))]，将真的判别为假和将假的判别为真\n",
    "loss_D = loss_data + loss_g\n",
    "\n",
    "#同样使用交叉熵构建生成器损失函数\n",
    "loss_G = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake)))\n",
    "\n",
    "#定义判别器和生成器的优化方法为Adam算法，关键字var_list表明最小化损失函数所更新的权重矩阵\n",
    "training_op_d = tf.train.AdamOptimizer().minimize(loss_D, var_list=theta_D)\n",
    "training_op_g = tf.train.AdamOptimizer().minimize(loss_G, var_list=theta_G)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "n_epochs = 20000\n",
    "batch_size = 128\n",
    "z_dim = 100\n",
    "\n",
    "checkpoint_path = \"/tmp/my_mnistGAN_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_mnistGAN_model\"\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "if not os.path.exists('out/'):\n",
    "    os.makedirs('out/')\n",
    "    \n",
    "#CREATE PHOTO\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\nD loss: 1.603\nG_loss: 2.079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 2000\nD loss: 0.1213\nG_loss: 3.874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4000\nD loss: 0.2483\nG_loss: 4.692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 6000\nD loss: 0.3041\nG_loss: 4.098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 8000\nD loss: 0.5982\nG_loss: 2.967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 10000\nD loss: 0.511\nG_loss: 3.484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 12000\nD loss: 0.6221\nG_loss: 2.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 14000\nD loss: 0.7866\nG_loss: 2.576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 16000\nD loss: 0.5479\nG_loss: 2.429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 18000\nD loss: 0.774\nG_loss: 2.441\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # if not os.path.exists(checkpoint_epoch_path):\n",
    "    #     start_epoch = 0\n",
    "    #     sess.run(init)\n",
    "    # else:\n",
    "    #     with open(checkpoint_epoch_path, 'rb') as f:\n",
    "    #         start_epoch = int(f.read())\n",
    "    #     print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "    #     saver.restore(sess, checkpoint_path)\n",
    "    i = 0 \n",
    "    # for epoch in range(start_epoch, n_epochs):\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        \n",
    "        X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        # Z_batch = sample_Z(batch_size, z_dim)\n",
    "        _, loss_D_val = sess.run([training_op_d, loss_D], feed_dict={X: X_batch, Z: sample_Z(batch_size, z_dim)})\n",
    "        _, loss_G_val = sess.run([training_op_g, loss_G], feed_dict={Z: sample_Z(batch_size, z_dim)})\n",
    "        \n",
    "        if epoch % 2000 == 0:\n",
    "            samples = sess.run(g_proba, feed_dict={Z: sample_Z(16, z_dim)})\n",
    "            fig = plot(samples)\n",
    "            plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "            i += 1\n",
    "            plt.close(fig)\n",
    "            \n",
    "        # if epoch % 5000 == 0:\n",
    "        #     with open(checkpoint_epoch_path, 'wb') as f:\n",
    "        #         saver.save(sess, checkpoint_path)\n",
    "        #         f.write(b'%d' % (epoch + 1))\n",
    "        \n",
    "        if epoch % 2000 == 0:\n",
    "            print('Iter: {}'.format(epoch))\n",
    "            print('D loss: {:.4}'. format(loss_D_val))\n",
    "            print('G_loss: {:.4}'.format(loss_G_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\nExtracting /tmp/data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "n_D_inputs = 28*28\n",
    "n_G_inputs = 100\n",
    "n_D_outputs = 1\n",
    "n_G_outputs = 28*28\n",
    "\n",
    "n_epochs = 20000\n",
    "batch_size = 128\n",
    "z_dim = 100\n",
    "\n",
    "checkpoint_path = \"/tmp/my_mnistGAN_model.ckpt\"\n",
    "checkpoint_epoch_path = checkpoint_path + \".epoch\"\n",
    "final_model_path = \"./my_mnistGAN_model\"\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def dnn(inputs, n_hidden_layers=1, n_neurons=128, pre_name=None, activation=tf.nn.elu,\n",
    "        initializer=he_init, reuse=None):\n",
    "    if pre_name == None:\n",
    "        pre_name = \"\"\n",
    "    with tf.name_scope(\"dnn\"):\n",
    "        for layer in range(n_hidden_layers):\n",
    "            inputs = tf.layers.dense(inputs, n_neurons, activation=activation, \\\n",
    "                    kernel_initializer=initializer, name=\"%shidden%d\" % (pre_name, layer + 1), reuse=reuse)\n",
    "    return inputs\n",
    "\n",
    "def generator(Z):\n",
    "    g_latent_out = dnn(Z, pre_name=\"g_\")\n",
    "    g_logits = tf.layers.dense(g_latent_out, n_G_outputs, kernel_initializer=he_init, name=\"g_logits\")\n",
    "    g_proba = tf.nn.sigmoid(g_logits, name=\"g_proba\")\n",
    "    return g_logits, g_proba\n",
    "\n",
    "def discriminator(X, reuse=None):\n",
    "    d_latent_out = dnn(X, pre_name=\"d_\",reuse=reuse)\n",
    "    d_logits = tf.layers.dense(d_latent_out, n_D_outputs, kernel_initializer=he_init, name=\"d_logits\", reuse=reuse)\n",
    "    d_proba = tf.nn.sigmoid(d_logits)\n",
    "    return d_logits, d_proba\n",
    "\n",
    "#CREATE PHOTO\n",
    "def plot(samples):\n",
    "    fig = plt.figure(figsize=(4, 4))\n",
    "    gs = gridspec.GridSpec(4, 4)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "    return fig\n",
    "\n",
    "def main():\n",
    "    reset_graph()\n",
    "    Z = tf.placeholder(tf.float32, shape=(None, n_G_inputs), name=\"Z\")\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_D_inputs), name=\"X\")\n",
    "\n",
    "    g_logits, g_proba = generator(Z)\n",
    "    d_logits_data, d_proba_data = discriminator(X)\n",
    "\n",
    "    d_logits_g, d_proba_g = discriminator(g_proba, reuse=True)\n",
    "\n",
    "    Variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    theta_G = Variables[:4]\n",
    "    theta_D = Variables[4:]\n",
    "\n",
    "    xentropy_g = tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_g, labels=tf.zeros_like(d_logits_g))\n",
    "    loss_g = tf.reduce_mean(xentropy_g, name=\"loss_g\")\n",
    "    xentropy_data = tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_data, labels=tf.ones_like(d_logits_data))\n",
    "    loss_data = tf.reduce_mean(xentropy_data, name=\"loss_data\")\n",
    "    loss_D = loss_data + loss_g\n",
    "    xentropy_G = tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_g, labels=tf.ones_like(d_logits_g))\n",
    "    loss_G = tf.reduce_mean(xentropy_G, name=\"loss_G\")\n",
    "\n",
    "    training_op_d = tf.train.AdamOptimizer().minimize(loss_D, var_list=theta_D)\n",
    "    training_op_g = tf.train.AdamOptimizer().minimize(loss_G, var_list=theta_G)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        if not os.path.exists('out/'):\n",
    "            os.makedirs('out/')\n",
    "\n",
    "        # if not os.path.exists(checkpoint_epoch_path):\n",
    "        #     start_epoch = 0\n",
    "        #     sess.run(init)\n",
    "        # else:\n",
    "        #     with open(checkpoint_epoch_path, 'rb') as f:\n",
    "        #         start_epoch = int(f.read())\n",
    "        #     print(\"Training was interrupted. Continuing at epoch\", start_epoch)\n",
    "        #     saver.restore(sess, checkpoint_path)\n",
    "        i = 0\n",
    "        # for epoch in range(start_epoch, n_epochs):\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            # Z_batch = sample_Z(batch_size, z_dim)\n",
    "            _, loss_D_val = sess.run([training_op_d, loss_D], feed_dict={X: X_batch, Z: sample_Z(batch_size, z_dim)})\n",
    "            _, loss_G_val = sess.run([training_op_g, loss_G], feed_dict={Z: sample_Z(batch_size, z_dim)})\n",
    "\n",
    "            if epoch % 2000 == 0:\n",
    "                samples = sess.run(g_proba, feed_dict={Z: sample_Z(16, z_dim)})\n",
    "                fig = plot(samples)\n",
    "                plt.savefig('out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "                i += 1\n",
    "                plt.close(fig)\n",
    "\n",
    "            # if epoch % 5000 == 0:\n",
    "            #     with open(checkpoint_epoch_path, 'wb') as f:\n",
    "            #         saver.save(sess, checkpoint_path)\n",
    "            #         f.write(b'%d' % (epoch + 1))\n",
    "\n",
    "            if epoch % 2000 == 0:\n",
    "                print('Iter: {}'.format(epoch))\n",
    "                print('D loss: {:.4}'.format(loss_D_val))\n",
    "                print('G_loss: {:.4}'.format(loss_G_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\nD loss: 1.875\nG_loss: 2.783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 2000\nD loss: 0.2635\nG_loss: 3.857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4000\nD loss: 0.5207\nG_loss: 3.113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 6000\nD loss: 0.4477\nG_loss: 3.209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 8000\nD loss: 0.2436\nG_loss: 3.115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 10000\nD loss: 0.3834\nG_loss: 2.934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 12000\nD loss: 0.3561\nG_loss: 2.891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 14000\nD loss: 0.4913\nG_loss: 2.904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 16000\nD loss: 0.6247\nG_loss: 2.594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 18000\nD loss: 0.5452\nG_loss: 2.681\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
